sample.twitter <- iconv(sample.twitter, "latin1", "ASCII", sub="")
sample.news <- iconv(sample.news, "latin1", "ASCII", sub="")
# Chunk 19
profanity.words <- readLines("profanity.txt")
corpus.blogs <- Corpus(VectorSource(list(sample.blogs)))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(tolower))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(removePunctuation))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(removeNumbers))
corpus.blogs <- tm_map(corpus.blogs, removeWords, stopwords("english"))
corpus.blogs <- tm_map(corpus.blogs, removeWords, profanity.words)
corpus.blogs <- tm_map(corpus.blogs, stripWhitespace)
corpus.blogs <- tm_map(corpus.blogs, stemDocument, language='english')
# Chunk 20
corpus.twitter <- Corpus(VectorSource(list(sample.twitter)))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(tolower))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(removePunctuation))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(removeNumbers))
corpus.twitter <- tm_map(corpus.twitter, removeWords, stopwords("english"))
corpus.twitter <- tm_map(corpus.twitter, removeWords, profanity.words)
corpus.twitter <- tm_map(corpus.twitter, stripWhitespace)
corpus.twitter <- tm_map(corpus.twitter, stemDocument, language='english')
# Chunk 21
corpus.news <- Corpus(VectorSource(list(sample.news)))
corpus.news <- tm_map(corpus.news, content_transformer(tolower))
corpus.news <- tm_map(corpus.news, content_transformer(removePunctuation))
corpus.news <- tm_map(corpus.news, content_transformer(removeNumbers))
corpus.news <- tm_map(corpus.news, removeWords, stopwords("english"))
corpus.news <- tm_map(corpus.news, removeWords, profanity.words)
corpus.news <- tm_map(corpus.news, stripWhitespace)
corpus.news <- tm_map(corpus.news, stemDocument, language='english')
# Chunk 22
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) #Define the unigram function
unigram.termdocmatrix.blogs <- TermDocumentMatrix(corpus.blogs, control = list(tokenize = UnigramTokenizer)) #generate unigrams
unigram.df.blogs <- data.frame(Term = unigram.termdocmatrix.blogs$dimnames$Terms,
Freq = unigram.termdocmatrix.blogs$v) #transform into dataframe
unigram.df.blogs <- unigram.df.blogs[order(unigram.df.blogs$Freq,decreasing = T),] # reorder dataframe
ggplot(head(unigram.df.blogs,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="yellow") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nBlogs") +
ylab("Frequency") +
xlab("Term")
# Chunk 23
cumsum.blogs <- cumsum(unigram.df.blogs$Freq)
limit.50.blogs <- sum(unigram.df.blogs$Freq)*0.5
length((cumsum.blogs[cumsum.blogs <=  limit.50.blogs]))
# Chunk 24
limit.90.blogs <- sum(unigram.df.blogs$Freq)*0.9
length((cumsum.blogs[cumsum.blogs <=  limit.90.blogs]))
# Chunk 25
unigram.termdocmatrix.twitter <- TermDocumentMatrix(corpus.twitter, control = list(tokenize = UnigramTokenizer)) #generate unigrams
unigram.df.twitter <- data.frame(Term = unigram.termdocmatrix.twitter$dimnames$Terms,
Freq = unigram.termdocmatrix.twitter$v) #transform into dataframe
unigram.df.twitter <- unigram.df.twitter[order(unigram.df.twitter$Freq,decreasing = T),] # reorder dataframe
ggplot(head(unigram.df.twitter,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="blue") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nTwitter") +
ylab("Frequency") +
xlab("Term")
# Chunk 26
cumsum.twitter <- cumsum(unigram.df.twitter$Freq)
limit.50.twitter <- sum(unigram.df.twitter$Freq)*0.5
length((cumsum.twitter[cumsum.twitter <=  limit.50.twitter]))
# Chunk 27
limit.90.twitter <- sum(unigram.df.twitter$Freq)*0.9
length((cumsum.twitter[cumsum.twitter <=  limit.90.twitter]))
# Chunk 28
unigram.termdocmatrix.news <- TermDocumentMatrix(corpus.news, control = list(tokenize = UnigramTokenizer)) #generate unigrams
unigram.df.news <- data.frame(Term = unigram.termdocmatrix.news$dimnames$Terms,
Freq = unigram.termdocmatrix.news$v) #transform into dataframe
unigram.df.news <- unigram.df.news[order(unigram.df.news$Freq,decreasing = T),] # reorder dataframe
ggplot(head(unigram.df.news,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="red") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nNews") +
ylab("Frequency") +
xlab("Term")
# Chunk 29
cumsum.news <- cumsum(unigram.df.news$Freq)
limit.50.news <- sum(unigram.df.news$Freq)*0.5
length((cumsum.news[cumsum.news <=  limit.50.news]))
# Chunk 30
limit.90.news <- sum(unigram.df.news$Freq)*0.9
length((cumsum.news[cumsum.news <=  limit.90.news]))
# Chunk 31
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) #Define the bigram function
bigram.termdocmatrix.blogs <- TermDocumentMatrix(corpus.blogs, control = list(tokenize = BigramTokenizer)) #generate bigrams
bigram.df.blogs <- data.frame(Term = bigram.termdocmatrix.blogs$dimnames$Terms,
Freq = bigram.termdocmatrix.blogs$v) #transform into dataframe
bigram.df.blogs <- bigram.df.blogs[order(bigram.df.blogs$Freq,decreasing = T),] # reorder dataframe
ggplot(head(bigram.df.blogs,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="yellow") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nBlogs") +
ylab("Frequency") +
xlab("Term")
# Chunk 32
cumsum.blogs <- cumsum(bigram.df.blogs$Freq)
limit.50.blogs <- sum(bigram.df.blogs$Freq)*0.5
length((cumsum.blogs[cumsum.blogs <=  limit.50.blogs]))
# Chunk 33
limit.90.blogs <- sum(bigram.df.blogs$Freq)*0.9
length((cumsum.blogs[cumsum.blogs <=  limit.90.blogs]))
# Chunk 34
bigram.termdocmatrix.twitter <- TermDocumentMatrix(corpus.twitter, control = list(tokenize = BigramTokenizer)) #generate bigrams
bigram.df.twitter <- data.frame(Term = bigram.termdocmatrix.twitter$dimnames$Terms,
Freq = bigram.termdocmatrix.twitter$v) #transform into dataframe
bigram.df.twitter <- bigram.df.twitter[order(bigram.df.twitter$Freq,decreasing = T),] # reorder dataframe
ggplot(head(bigram.df.twitter,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="blue") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nTwitter") +
ylab("Frequency") +
xlab("Term")
# Chunk 35
cumsum.twitter <- cumsum(bigram.df.twitter$Freq)
limit.50.twitter <- sum(bigram.df.twitter$Freq)*0.5
length((cumsum.twitter[cumsum.twitter <=  limit.50.twitter]))
# Chunk 36
limit.90.twitter <- sum(bigram.df.twitter$Freq)*0.9
length((cumsum.twitter[cumsum.twitter <=  limit.90.twitter]))
# Chunk 37
bigram.termdocmatrix.news <- TermDocumentMatrix(corpus.news, control = list(tokenize = BigramTokenizer)) #generate bigrams
bigram.df.news <- data.frame(Term = bigram.termdocmatrix.news$dimnames$Terms,
Freq = bigram.termdocmatrix.news$v) #transform into dataframe
bigram.df.news <- bigram.df.news[order(bigram.df.news$Freq,decreasing = T),] # reorder dataframe
ggplot(head(bigram.df.news,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="red") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nNews") +
ylab("Frequency") +
xlab("Term")
# Chunk 38
cumsum.news <- cumsum(bigram.df.news$Freq)
limit.50.news <- sum(bigram.df.news$Freq)*0.5
length((cumsum.news[cumsum.news <=  limit.50.news]))
# Chunk 39
limit.90.news <- sum(bigram.df.news$Freq)*0.9
length((cumsum.news[cumsum.news <=  limit.90.news]))
# Chunk 1: load_libraries
library(stringi)
library(tm)
library(RWeka)
library(ggplot2)
library(dplyr)
# Chunk 2: data_content
list.files("data/final")
# Chunk 3: english_content
list.files("data/final/en_US")
# Chunk 4: basic_analysis
blog.file <- "data/final/en_US/en_US.blogs.txt"
twitter.file <- "data/final/en_US/en_US.twitter.txt"
news.file <- "data/final/en_US/en_US.news.txt"
blogs <- readLines(blog.file, encoding="UTF-8")
twitter <- readLines(twitter.file, encoding="UTF-8")
news  <- readLines(news.file, encoding="UTF-8")
# Chunk 5: blog_lines
data.frame(t(stri_stats_general(blogs)))
# Chunk 6: blog_summary
summary(sapply(blogs,FUN=nchar))
# Chunk 7: blog_wordcount
summary(stri_count_words(blogs))
# Chunk 8: twitter_ines
data.frame(t(stri_stats_general(twitter)))
# Chunk 9: twitter_summary
summary(sapply(twitter,FUN=nchar))
# Chunk 10: twitter_wordcount
summary(stri_count_words(twitter))
# Chunk 11: news_line
data.frame(t(stri_stats_general(news)))
# Chunk 12: news_summary
summary(sapply(news,FUN=nchar))
# Chunk 13: news_wordcount
summary(stri_count_words(news))
# Chunk 14: sample_size
sample.size <- 100000
sample.blogs <- sample(blogs,sample.size)
sample.twitter <- sample(twitter,sample.size)
sample.news <- sample(news,sample.size)
# Chunk 15: save_samples
save(sample.blogs,sample.twitter,sample.news,file="samples.RData")
# Chunk 16: load_samples
load("samples.RData")
# Chunk 17: data_cleanup
sample.blogs <- iconv(sample.blogs, "latin1", "ASCII", sub="")
sample.twitter <- iconv(sample.twitter, "latin1", "ASCII", sub="")
sample.news <- iconv(sample.news, "latin1", "ASCII", sub="")
# Chunk 18: blogs_cleanup
profanity.words <- readLines("profanity.txt")
corpus.blogs <- Corpus(VectorSource(list(sample.blogs)))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(tolower))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(removePunctuation))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(removeNumbers))
corpus.blogs <- tm_map(corpus.blogs, removeWords, stopwords("english"))
corpus.blogs <- tm_map(corpus.blogs, removeWords, profanity.words)
corpus.blogs <- tm_map(corpus.blogs, stripWhitespace)
corpus.blogs <- tm_map(corpus.blogs, stemDocument, language='english')
# Chunk 19: twitter_cleanup
corpus.twitter <- Corpus(VectorSource(list(sample.twitter)))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(tolower))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(removePunctuation))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(removeNumbers))
corpus.twitter <- tm_map(corpus.twitter, removeWords, stopwords("english"))
corpus.twitter <- tm_map(corpus.twitter, removeWords, profanity.words)
corpus.twitter <- tm_map(corpus.twitter, stripWhitespace)
corpus.twitter <- tm_map(corpus.twitter, stemDocument, language='english')
# Chunk 20: news_cleanup
corpus.news <- Corpus(VectorSource(list(sample.news)))
corpus.news <- tm_map(corpus.news, content_transformer(tolower))
corpus.news <- tm_map(corpus.news, content_transformer(removePunctuation))
corpus.news <- tm_map(corpus.news, content_transformer(removeNumbers))
corpus.news <- tm_map(corpus.news, removeWords, stopwords("english"))
corpus.news <- tm_map(corpus.news, removeWords, profanity.words)
corpus.news <- tm_map(corpus.news, stripWhitespace)
corpus.news <- tm_map(corpus.news, stemDocument, language='english')
# Chunk 21: blogs_unigram
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram.termdocmatrix.blogs <- TermDocumentMatrix(corpus.blogs, control = list(tokenize = UnigramTokenizer))
unigram.df.blogs <- data.frame(Term = unigram.termdocmatrix.blogs$dimnames$Terms,
Freq = unigram.termdocmatrix.blogs$v)
unigram.df.blogs <- unigram.df.blogs[order(unigram.df.blogs$Freq,decreasing = T),]
ggplot(head(unigram.df.blogs,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="yellow") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nBlogs") +
ylab("Frequency") +
xlab("Term")
# Chunk 22: blogs_fifty
cumsum.blogs <- cumsum(unigram.df.blogs$Freq)
limit.50.blogs <- sum(unigram.df.blogs$Freq)*0.5
length((cumsum.blogs[cumsum.blogs <=  limit.50.blogs]))
# Chunk 23: blogs_ninety
limit.90.blogs <- sum(unigram.df.blogs$Freq)*0.9
length((cumsum.blogs[cumsum.blogs <=  limit.90.blogs]))
# Chunk 24: twitter_unigram
unigram.termdocmatrix.twitter <- TermDocumentMatrix(corpus.twitter, control = list(tokenize = UnigramTokenizer))
unigram.df.twitter <- data.frame(Term = unigram.termdocmatrix.twitter$dimnames$Terms,
Freq = unigram.termdocmatrix.twitter$v)
unigram.df.twitter <- unigram.df.twitter[order(unigram.df.twitter$Freq,decreasing = T),]
ggplot(head(unigram.df.twitter,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="blue") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nTwitter") +
ylab("Frequency") +
xlab("Term")
# Chunk 25: twitter_fifty
cumsum.twitter <- cumsum(unigram.df.twitter$Freq)
limit.50.twitter <- sum(unigram.df.twitter$Freq)*0.5
length((cumsum.twitter[cumsum.twitter <=  limit.50.twitter]))
# Chunk 26: twitter_ninety
limit.90.twitter <- sum(unigram.df.twitter$Freq)*0.9
length((cumsum.twitter[cumsum.twitter <=  limit.90.twitter]))
# Chunk 27: news_unigram
unigram.termdocmatrix.news <- TermDocumentMatrix(corpus.news, control = list(tokenize = UnigramTokenizer))
unigram.df.news <- data.frame(Term = unigram.termdocmatrix.news$dimnames$Terms,
Freq = unigram.termdocmatrix.news$v)
unigram.df.news <- unigram.df.news[order(unigram.df.news$Freq,decreasing = T),]
ggplot(head(unigram.df.news,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="red") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nNews") +
ylab("Frequency") +
xlab("Term")
# Chunk 28: news_fifty
cumsum.news <- cumsum(unigram.df.news$Freq)
limit.50.news <- sum(unigram.df.news$Freq)*0.5
length((cumsum.news[cumsum.news <=  limit.50.news]))
# Chunk 29: news_ninety
limit.90.news <- sum(unigram.df.news$Freq)*0.9
length((cumsum.news[cumsum.news <=  limit.90.news]))
# Chunk 30: blogs_bigrams
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram.termdocmatrix.blogs <- TermDocumentMatrix(corpus.blogs, control = list(tokenize = BigramTokenizer))
bigram.df.blogs <- data.frame(Term = bigram.termdocmatrix.blogs$dimnames$Terms,
Freq = bigram.termdocmatrix.blogs$v)
bigram.df.blogs <- bigram.df.blogs[order(bigram.df.blogs$Freq,decreasing = T),]
ggplot(head(bigram.df.blogs,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="yellow") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nBlogs") +
ylab("Frequency") +
xlab("Term")
# Chunk 31: blogs_bigrams_fifty
cumsum.blogs <- cumsum(bigram.df.blogs$Freq)
limit.50.blogs <- sum(bigram.df.blogs$Freq)*0.5
length((cumsum.blogs[cumsum.blogs <=  limit.50.blogs]))
# Chunk 32: blogs_bigrams_ninety
limit.90.blogs <- sum(bigram.df.blogs$Freq)*0.9
length((cumsum.blogs[cumsum.blogs <=  limit.90.blogs]))
# Chunk 33: twitter_bigrams
bigram.termdocmatrix.twitter <- TermDocumentMatrix(corpus.twitter, control = list(tokenize = BigramTokenizer))
bigram.df.twitter <- data.frame(Term = bigram.termdocmatrix.twitter$dimnames$Terms,
Freq = bigram.termdocmatrix.twitter$v) dataframe
bigram.df.twitter <- bigram.df.twitter[order(bigram.df.twitter$Freq,decreasing = T),]
ggplot(head(bigram.df.twitter,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="blue") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nTwitter") +
ylab("Frequency") +
xlab("Term")
# Chunk 34: twitter_bigrams_fifty
cumsum.twitter <- cumsum(bigram.df.twitter$Freq)
limit.50.twitter <- sum(bigram.df.twitter$Freq)*0.5
length((cumsum.twitter[cumsum.twitter <=  limit.50.twitter]))
# Chunk 35: twitter_bigrams_ninety
limit.90.twitter <- sum(bigram.df.twitter$Freq)*0.9
length((cumsum.twitter[cumsum.twitter <=  limit.90.twitter]))
# Chunk 36: news_bigrams
bigram.termdocmatrix.news <- TermDocumentMatrix(corpus.news, control = list(tokenize = BigramTokenizer))
bigram.df.news <- data.frame(Term = bigram.termdocmatrix.news$dimnames$Terms,
Freq = bigram.termdocmatrix.news$v)
bigram.df.news <- bigram.df.news[order(bigram.df.news$Freq,decreasing = T),]
ggplot(head(bigram.df.news,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="red") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nNews") +
ylab("Frequency") +
xlab("Term")
# Chunk 37: news_bigrams_fifty
cumsum.news <- cumsum(bigram.df.news$Freq)
limit.50.news <- sum(bigram.df.news$Freq)*0.5
length((cumsum.news[cumsum.news <=  limit.50.news]))
# Chunk 38: news_bigrams_ninety
limit.90.news <- sum(bigram.df.news$Freq)*0.9
length((cumsum.news[cumsum.news <=  limit.90.news]))
# Chunk 1: load_libraries
library(stringi)
library(tm)
library(RWeka)
library(ggplot2)
library(dplyr)
# Chunk 2: data_content
list.files("data/final")
# Chunk 3: english_content
list.files("data/final/en_US")
# Chunk 4: basic_analysis
blog.file <- "data/final/en_US/en_US.blogs.txt"
twitter.file <- "data/final/en_US/en_US.twitter.txt"
news.file <- "data/final/en_US/en_US.news.txt"
blogs <- readLines(blog.file, encoding="UTF-8")
twitter <- readLines(twitter.file, encoding="UTF-8")
news  <- readLines(news.file, encoding="UTF-8")
# Chunk 5: blog_lines
data.frame(t(stri_stats_general(blogs)))
# Chunk 6: blog_summary
summary(sapply(blogs,FUN=nchar))
# Chunk 7: blog_wordcount
summary(stri_count_words(blogs))
# Chunk 8: twitter_ines
data.frame(t(stri_stats_general(twitter)))
# Chunk 9: twitter_summary
summary(sapply(twitter,FUN=nchar))
# Chunk 10: twitter_wordcount
summary(stri_count_words(twitter))
# Chunk 11: news_line
data.frame(t(stri_stats_general(news)))
# Chunk 12: news_summary
summary(sapply(news,FUN=nchar))
# Chunk 13: news_wordcount
summary(stri_count_words(news))
# Chunk 14: sample_size
sample.size <- 100000
sample.blogs <- sample(blogs,sample.size)
sample.twitter <- sample(twitter,sample.size)
sample.news <- sample(news,sample.size)
# Chunk 15: save_samples
save(sample.blogs,sample.twitter,sample.news,file="samples.RData")
# Chunk 16: load_samples
load("samples.RData")
# Chunk 17: data_cleanup
sample.blogs <- iconv(sample.blogs, "latin1", "ASCII", sub="")
sample.twitter <- iconv(sample.twitter, "latin1", "ASCII", sub="")
sample.news <- iconv(sample.news, "latin1", "ASCII", sub="")
# Chunk 18: blogs_cleanup
profanity.words <- readLines("profanity.txt")
corpus.blogs <- Corpus(VectorSource(list(sample.blogs)))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(tolower))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(removePunctuation))
corpus.blogs <- tm_map(corpus.blogs, content_transformer(removeNumbers))
corpus.blogs <- tm_map(corpus.blogs, removeWords, stopwords("english"))
corpus.blogs <- tm_map(corpus.blogs, removeWords, profanity.words)
corpus.blogs <- tm_map(corpus.blogs, stripWhitespace)
corpus.blogs <- tm_map(corpus.blogs, stemDocument, language='english')
# Chunk 19: twitter_cleanup
corpus.twitter <- Corpus(VectorSource(list(sample.twitter)))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(tolower))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(removePunctuation))
corpus.twitter <- tm_map(corpus.twitter, content_transformer(removeNumbers))
corpus.twitter <- tm_map(corpus.twitter, removeWords, stopwords("english"))
corpus.twitter <- tm_map(corpus.twitter, removeWords, profanity.words)
corpus.twitter <- tm_map(corpus.twitter, stripWhitespace)
corpus.twitter <- tm_map(corpus.twitter, stemDocument, language='english')
# Chunk 20: news_cleanup
corpus.news <- Corpus(VectorSource(list(sample.news)))
corpus.news <- tm_map(corpus.news, content_transformer(tolower))
corpus.news <- tm_map(corpus.news, content_transformer(removePunctuation))
corpus.news <- tm_map(corpus.news, content_transformer(removeNumbers))
corpus.news <- tm_map(corpus.news, removeWords, stopwords("english"))
corpus.news <- tm_map(corpus.news, removeWords, profanity.words)
corpus.news <- tm_map(corpus.news, stripWhitespace)
corpus.news <- tm_map(corpus.news, stemDocument, language='english')
# Chunk 21: blogs_unigram
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram.termdocmatrix.blogs <- TermDocumentMatrix(corpus.blogs, control = list(tokenize = UnigramTokenizer))
unigram.df.blogs <- data.frame(Term = unigram.termdocmatrix.blogs$dimnames$Terms,
Freq = unigram.termdocmatrix.blogs$v)
unigram.df.blogs <- unigram.df.blogs[order(unigram.df.blogs$Freq,decreasing = T),]
ggplot(head(unigram.df.blogs,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="yellow") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nBlogs") +
ylab("Frequency") +
xlab("Term")
# Chunk 22: blogs_fifty
cumsum.blogs <- cumsum(unigram.df.blogs$Freq)
limit.50.blogs <- sum(unigram.df.blogs$Freq)*0.5
length((cumsum.blogs[cumsum.blogs <=  limit.50.blogs]))
# Chunk 23: blogs_ninety
limit.90.blogs <- sum(unigram.df.blogs$Freq)*0.9
length((cumsum.blogs[cumsum.blogs <=  limit.90.blogs]))
# Chunk 24: twitter_unigram
unigram.termdocmatrix.twitter <- TermDocumentMatrix(corpus.twitter, control = list(tokenize = UnigramTokenizer))
unigram.df.twitter <- data.frame(Term = unigram.termdocmatrix.twitter$dimnames$Terms,
Freq = unigram.termdocmatrix.twitter$v)
unigram.df.twitter <- unigram.df.twitter[order(unigram.df.twitter$Freq,decreasing = T),]
ggplot(head(unigram.df.twitter,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="blue") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nTwitter") +
ylab("Frequency") +
xlab("Term")
# Chunk 25: twitter_fifty
cumsum.twitter <- cumsum(unigram.df.twitter$Freq)
limit.50.twitter <- sum(unigram.df.twitter$Freq)*0.5
length((cumsum.twitter[cumsum.twitter <=  limit.50.twitter]))
# Chunk 26: twitter_ninety
limit.90.twitter <- sum(unigram.df.twitter$Freq)*0.9
length((cumsum.twitter[cumsum.twitter <=  limit.90.twitter]))
# Chunk 27: news_unigram
unigram.termdocmatrix.news <- TermDocumentMatrix(corpus.news, control = list(tokenize = UnigramTokenizer))
unigram.df.news <- data.frame(Term = unigram.termdocmatrix.news$dimnames$Terms,
Freq = unigram.termdocmatrix.news$v)
unigram.df.news <- unigram.df.news[order(unigram.df.news$Freq,decreasing = T),]
ggplot(head(unigram.df.news,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="red") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Unigrams frequency\nNews") +
ylab("Frequency") +
xlab("Term")
# Chunk 28: news_fifty
cumsum.news <- cumsum(unigram.df.news$Freq)
limit.50.news <- sum(unigram.df.news$Freq)*0.5
length((cumsum.news[cumsum.news <=  limit.50.news]))
# Chunk 29: news_ninety
limit.90.news <- sum(unigram.df.news$Freq)*0.9
length((cumsum.news[cumsum.news <=  limit.90.news]))
# Chunk 30: blogs_bigrams
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram.termdocmatrix.blogs <- TermDocumentMatrix(corpus.blogs, control = list(tokenize = BigramTokenizer))
bigram.df.blogs <- data.frame(Term = bigram.termdocmatrix.blogs$dimnames$Terms,
Freq = bigram.termdocmatrix.blogs$v)
bigram.df.blogs <- bigram.df.blogs[order(bigram.df.blogs$Freq,decreasing = T),]
ggplot(head(bigram.df.blogs,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="yellow") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nBlogs") +
ylab("Frequency") +
xlab("Term")
# Chunk 31: blogs_bigrams_fifty
cumsum.blogs <- cumsum(bigram.df.blogs$Freq)
limit.50.blogs <- sum(bigram.df.blogs$Freq)*0.5
length((cumsum.blogs[cumsum.blogs <=  limit.50.blogs]))
# Chunk 32: blogs_bigrams_ninety
limit.90.blogs <- sum(bigram.df.blogs$Freq)*0.9
length((cumsum.blogs[cumsum.blogs <=  limit.90.blogs]))
# Chunk 33: twitter_bigrams
bigram.termdocmatrix.twitter <- TermDocumentMatrix(corpus.twitter, control = list(tokenize = BigramTokenizer))
bigram.df.twitter <- data.frame(Term = bigram.termdocmatrix.twitter$dimnames$Terms,
Freq = bigram.termdocmatrix.twitter$v)
bigram.df.twitter <- bigram.df.twitter[order(bigram.df.twitter$Freq,decreasing = T),]
ggplot(head(bigram.df.twitter,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="blue") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nTwitter") +
ylab("Frequency") +
xlab("Term")
# Chunk 34: twitter_bigrams_fifty
cumsum.twitter <- cumsum(bigram.df.twitter$Freq)
limit.50.twitter <- sum(bigram.df.twitter$Freq)*0.5
length((cumsum.twitter[cumsum.twitter <=  limit.50.twitter]))
# Chunk 35: twitter_bigrams_ninety
limit.90.twitter <- sum(bigram.df.twitter$Freq)*0.9
length((cumsum.twitter[cumsum.twitter <=  limit.90.twitter]))
# Chunk 36: news_bigrams
bigram.termdocmatrix.news <- TermDocumentMatrix(corpus.news, control = list(tokenize = BigramTokenizer))
bigram.df.news <- data.frame(Term = bigram.termdocmatrix.news$dimnames$Terms,
Freq = bigram.termdocmatrix.news$v)
bigram.df.news <- bigram.df.news[order(bigram.df.news$Freq,decreasing = T),]
ggplot(head(bigram.df.news,10), aes(x=reorder(Term,-Freq), y=Freq)) +
geom_bar(stat="Identity", fill="red") +
geom_text(aes(label=Freq), vjust = -0.5) +
ggtitle("Bigrams frequency\nNews") +
ylab("Frequency") +
xlab("Term")
# Chunk 37: news_bigrams_fifty
cumsum.news <- cumsum(bigram.df.news$Freq)
limit.50.news <- sum(bigram.df.news$Freq)*0.5
length((cumsum.news[cumsum.news <=  limit.50.news]))
# Chunk 38: news_bigrams_ninety
limit.90.news <- sum(bigram.df.news$Freq)*0.9
length((cumsum.news[cumsum.news <=  limit.90.news]))
